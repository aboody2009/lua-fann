{"note":"Don't delete this file! It's used internally to help with page regeneration.","google":"","name":"Lua-fann","body":"# LuaFann\r\n\r\nThese are a set of `Lua` bindings for the [Fast Artificial Neural Network (`FANN`) library](http://www.leenissen.dk/fann/).\r\n\r\nThe `FANN` project has an excellent [introduction](http://fann.sf.net/fann_en.pdf) on the theory and operation of artificial neural networks on its homepage.\r\n\r\nThe source code in this module is released under the `GNU Lesser General Public License` (`LGPL`), which is the license under which the `FANN` library is released.\r\n\r\nYou can access the [README](https://github.com/msva/lua-fann/blob/master/README.md) or the [manual](/lua-fann/doc/luafann.html) from this page.\r\n\r\n## Info\r\nCurrent Team Lead: `Vadim A. Misbakh-Soloviov` (@msva, mva@mva.name)'2012\r\n<br />\r\nOriginal Author: `Werner Stoop` (wstoop@gmail.com)'2009\r\n\r\nFeel free to contact me (@msva) if you have any problems/questions/suggestions. Alternatively, you can post your ideas/questions/suggestions on [issue tracker](https://github.com/msva/lua-fann/issues)\r\n\r\n## Building\r\n\r\nThe `LuaFann` module was written using `ANSI C`, and should work on any platforms supported by `Lua` and `FANN`. In order to build `LuaFann`, you should have `Lua` and `FANN` already installed on your system.\r\n\r\nIt is a `Makefile` (`GNU` one, but it should work on `BSD` and other POSIX-systems (not Windows) as well), which you can use to build LuaFann on your system. Look at README for help about targets.\r\n\r\nUnfortunately I don't have access to other operating systems supported by `Lua` and `FANN`, so I cannot provide specific makefiles/project files for those. If you can contribute it would be greatly appreciated. If you need to build the module on another operating system you can follow the instructions found on the [Building Modules](http://lua-users.org/wiki/BuildingModules) page in the `Lua users wiki`.\r\n\r\n## Example\r\n\r\nHere is an example of a `Lua` script that imitates the `FANN` `XOR` example: `module.lua` (in test directory). This example trains a neural network to mimic the `exclusive-OR` (`XOR`) function (In the example, a `-1` is used for a boolean `false` and a `+1` is used for a boolean `true`).\r\n\r\nLoad the `LuaFann` module into `Lua` through the require statement:\r\n```lua\r\nrequire(\"fann\")\r\n```\r\nCreate a neural network object through the `FANN` `create_standard()` function. The example creates a neural network with three layers where there are 2 input neurons, 2 neurons in the hidden layer and one output layer neuron:\r\n```lua\r\nann = fann.create_standard(3, 2, 2, 1)\r\n```\r\nNext load some training data from a file through the `read_train_from_file()` function.\r\n```lua\r\ntrain = fann.read_train_from_file(\"xor.data\")\r\n```\r\nThe `xor.data` training file comes from the `FANN` examples and is described in more detail on the `FANN` homepage.\r\n```\r\n4 2 1\r\n-1 -1\r\n-1\r\n-1 1\r\n1\r\n1 -1\r\n1\r\n1 1\r\n-1\r\n```\r\nIt is a simple text file wherein the first row describes the training set: There are four entries, with two inputs and one output. Thereafter are alternating rows where the even rows' describe the entry's inputs and the odd rows describe the entry's output(s).\r\n\r\nThe second and third rows describe the first training set: The second row has `-1 -1` as the two input values and the third row contains `-1` as the output value (more sophisticated networks may have more outputs) which means that when both inputs are `false`, the neural network should be trained to output a `false`.\r\n\r\nLikewise the forth and fifth rows describe the second training set, the sixth and seventh rows describe the third training set and the eight and ninth rows describe the forth training set. You'll notice that this training set will teach the network to output `-1` when its inputs are the same and to output `1` when its inputs are different (thus a `XOR` function)\r\n\r\nNext, some other parameters of the network are set. The `FANN` documentation provides some more detail on these:\r\n```lua\r\nann:set_activation_steepness_hidden(1)\r\nann:set_activation_steepness_output(1)\r\nann:set_activation_function_hidden(fann.FANN_SIGMOID_SYMMETRIC)\r\nann:set_activation_function_output(fann.FANN_SIGMOID_SYMMETRIC)\r\nann:set_train_stop_function(fann.FANN_STOPFUNC_BIT)\r\nann:set_bit_fail_limit(0.01)\r\n```\r\nNow the weights within the neural network are initialized according to the training data:\r\n```lua\r\nann:init_weights(train)\r\n```\r\nYou are now ready to call the `train_on_data()` function to train the neural network:\r\n```lua\r\nann:train_on_data(train, 500000, 1000, 0.001)\r\n```\r\nYou can use the `test_data()` function to calculate the mean-square error, which gives an indication of how well the neural network performs. The closer to zero this value is the smaller the error in the trained neural network:\r\n```lua\r\nmse = ann:test_data(train)\r\nprint(\"MSE: \" .. mse)\r\n```\r\nYou can use the `save()` function to save a neural network to a file.\r\n```lua\r\nann:save(\"myxor.net\")\r\n```\r\nYou can reload the neural network at a later stage through the `create_from_file()` function:\r\n\r\n```lua\r\nann = fann.create_from_file(\"myxor.net\")\r\n```\r\nYou are now ready to use the trained neural network to classify inputs through the `run()` function. In this example the `run()` function takes two inputs and returns a single output (which corresponds to the training set; other networks may have more inputs or outputs):\r\n```lua\r\nxor = ann:run(1, 1)\r\nprint(\"Result: \" .. xor)\r\nxor = ann:run(1, -1)\r\nprint(\"Result: \" .. xor)\r\nxor = ann:run(-1, -1)\r\nprint(\"Result: \" .. xor)\r\nxor = ann:run(-1, 1)\r\nprint(\"Result: \" .. xor)\r\n```\r\nFor fun, try changing the inputs to the network slightly, using, say, `xor = ann:run(1.1, -0.75)` to see that the network will still output a valid result.\r\n## Future\r\nThere are still a lot of the more advanced `FANN` functions outstanding. I hope to get around to those eventually.\r\n\r\nIf there are functions you require urgently, you can place a feature request through the GitHub [issues](https://github.com/msva/lua-fann/issues/).\r\n\r\nNaturally, contributions from the community are always welcome.\r\n","tagline":"LuaFann: Lua bindings for the Fast Artificial Neural Network Library"}